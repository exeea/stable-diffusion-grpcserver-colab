{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/exeea/stable-diffusion-grpcserver-colab/blob/main/GRPCServer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Information"
      ],
      "metadata": {
        "id": "6-464OaL4AdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Obtain information about the runtime (gpu, drivers, VRAM, disk space)\n",
        "! nvidia-smi\n",
        "! nvcc -V\n",
        "! free -h\n",
        "!echo \"-------------------------------------------------------------------------------\"\n",
        "!df -h"
      ],
      "metadata": {
        "id": "gLgFVcXp4GGy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "yHCcGdeT3pv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## Install the Environment\n",
        "import os\n",
        "import sys\n",
        "!apt update\n",
        "!apt upgrade -y\n",
        "!python --version\n",
        "\n",
        "if not os.path.exists(\"/usr/local/bin/conda\"):\n",
        "  print(\"Installing conda...\")\n",
        "  !wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "  !chmod +x mini.sh\n",
        "  !bash ./mini.sh -b -f -p /usr/local\n",
        "  !rm mini.sh\n",
        "  !conda install -q -y jupyter\n",
        "  !conda install -q -y google-colab -c conda-forge\n",
        "  !python -m ipykernel install --name \"py39\" --user\n",
        "  !apt install python-pip -y\n",
        "  !python --version\n",
        "  print(\"Conda is ready.\")\n",
        "else:\n",
        "  print(\"Conda already installed.\")\n",
        "\n",
        "!conda update -n base -c defaults conda -y\n",
        "print(\"done.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aRg-h79j7ZWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX-ZbTMsGAu6"
      },
      "outputs": [],
      "source": [
        "# @markdown ## Install the GRPCServer\n",
        "# @markdown Note: xformers are not yet implemented in this colab.\n",
        "%cd /content\n",
        "!git clone --depth=1 https://github.com/hafriedlander/stable-diffusion-grpcserver.git\n",
        "%cd stable-diffusion-grpcserver\n",
        "!git submodule update --init --recursive\n",
        "os.environ['PIP_EXTRA_INDEX_URL'] = \"https://download.pytorch.org/whl/cu116\"\n",
        "os.environ['FLIT_ROOT_INSTALL'] = \"1\"\n",
        "\n",
        "install_script = \"\"\"#!/bin/bash\n",
        "  eval \"$(conda shell.bash hook)\"\n",
        "  cd /content/stable-diffusion-grpcserver\n",
        "  conda env create -f environment.yaml\n",
        "  conda activate sd-grpc-server\n",
        "  flit install --pth-file\n",
        "#  conda install -y xformers -c xformers/label/dev\n",
        "#  python3 -m pip install --yes --upgrade tensorrt\n",
        "  \"\"\"\n",
        "! {install_script}\n",
        "\n",
        "print(\"done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start service"
      ],
      "metadata": {
        "id": "LlOf-4sR_vCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ## Run script\n",
        "# @markdown keep in mind that this script is set to run forever.\n",
        "\n",
        "# @markdown The HuggingFace API Key can be obtained from https://huggingface.co/settings/tokens, if you don't have one: create it with \"write\" permissions.\n",
        "HUGGINGFACE_API_KEY = \"hf_your_huggingface_api_token\" # @param {type: \"string\"}\n",
        "STORE_ON_GOOGLEDRIVE = \"no\" #@param [\"no\",\"yes\"]\n",
        "# @markdown The Access Token is your secret API key that you will use to connect. This is up to you.\n",
        "ACCESS_TOKEN = \"your_secret\" # @param {type: \"string\"}\n",
        "NSFW_BEHAVIOUR = \"ignore\" #@param [\"ignore\",\"flag\",\"block\"]\n",
        "\n",
        "\n",
        "# @markdown üî• <ins>when the server is started, before the models are loaded, you will get a message similar to:\n",
        "# @markdown > GRPC listening on port 50051, HTTP listening on port 5000. Start your engines....\\\n",
        "# @markdown > Loading engines...\n",
        "# @markdown > - Engine stable-diffusion-v1-5-standard...\n",
        "# @markdown > - Model laion-clip-b...\\\n",
        "# @markdown > Localtunnel started. Use these settings to connect:\n",
        "# @markdown >>  Server 'this-is-an-example-54-121-190-94.loca.lt'\\\n",
        "# @markdown >>  Port '443'\\\n",
        "# @markdown >>  Key 'your_secret'\n",
        "# @markdown >>  \n",
        "# @markdown >\n",
        "# @markdown ## Use these settings to connect: Server, Port and Key\n",
        "\n",
        "import os\n",
        "if STORE_ON_GOOGLEDRIVE == \"yes\":\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/store/', force_remount=True)\n",
        "  os.environ['HF_HOME'] = '/content/store/MyDrive/grpcserver/huggingface';\n",
        "  os.environ['SD_WEIGHT_ROOT'] = '/content/store/MyDrive/grpcserver/weights';\n",
        "else:\n",
        "  os.environ['HF_HOME'] = '/content/huggingface';\n",
        "  os.environ['SD_WEIGHT_ROOT'] = '/content/weights';\n",
        "\n",
        "%cd /content/stable-diffusion-grpcserver\n",
        "! git pull\n",
        "!echo \"Machine IP: \"\n",
        "!curl ipecho.net/plain\n",
        "!echo \"\"\n",
        "os.environ['PIP_EXTRA_INDEX_URL'] = \"https://download.pytorch.org/whl/cu116\"\n",
        "os.environ['SD_LISTEN_TO_ALL'] = \"1\"\n",
        "os.environ['SD_LOCALTUNNEL'] = \"1\"\n",
        "os.environ['HF_API_TOKEN'] = HUGGINGFACE_API_KEY\n",
        "os.environ['SD_ACCESS_TOKEN'] = ACCESS_TOKEN\n",
        "os.environ['SD_NSFW_BEHAVIOUR'] = NSFW_BEHAVIOUR\n",
        "run_script = \"\"\"#!/bin/sh\n",
        "  eval \"$(conda shell.bash hook)\"\n",
        "  conda activate sd-grpc-server\n",
        "  python -V\n",
        "  python server.py\"\"\"\n",
        "! {run_script}"
      ],
      "metadata": {
        "id": "zCKRqLfyNYJk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö†Ô∏è Important ‚ö†Ô∏è\n",
        "### Before the first connection you will have to navigate, using your browser, to https://that_server_address_you_got_.loca.lt and press the CONTINUE button: this will allow your IP to use the endpoint.\n",
        "Until this operation is not executed, you will not be able to consume the API.\\\n",
        "This is a limitation/security imposed by the localtunnel.\n",
        "\n",
        "If you get the message with \"Forbidden Resources\", then all is good and ready to go!\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "#Notes:\n",
        "### idea2.art: \n",
        "- you have to fill up server, port and key as written (no http or https in front of the dns)\n",
        "\n",
        "### flying dog PS/Krita plugin: \n",
        "- you have to put https:// in front of the dns, for example https://flying-server-324.34lr4.loca.lt (the port is not needed, https is 443)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fO16NYMg0t_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GitHub: https://github.com/exeea/stable-diffusion-grpcserver-colab"
      ],
      "metadata": {
        "id": "nqRDJ7NaJ7lj"
      }
    }
  ]
}